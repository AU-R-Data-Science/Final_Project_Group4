---
title: "Logistic Regression Through Numerical Optimization"
author: "Final Group 4"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package Overview

This R package contains functions designed to perform logistic regression on a user's chosen response and predictor data. This is done through estimating the coefficient vector $\beta$ and which contains response and predictor data and the intercept. This is done using the function
$$
\hat{\beta}\::=\underset{\beta}{\operatorname{argmin}}\sum_{i=1}^{n}(-y_{i}\cdot\ln(p_{i})-(1-y_{i})\cdot\ln(1-p_{i}))
$$
where
$$
p_{i}\::=\frac{1}{1+exp(-x_{i}^{T}\beta)}
$$
with $y_{i}$ representing the $i^{\text{th}}$ observation of the response and $x_{i}$ representing the $i^{\text{th}}$ row of the predictors.

## Install Instructions
#### Repository: AU-R-Data-Science/Final_Project_Group4
#### Package: FinalProjectGroup4
```{r instr, include = TRUE, echo = TRUE}
library(devtools)
install_github("AU-R-Data-Science/Final_Project_Group4/FinalProjectGroup4")
library(FinalProjectGroup4)
```
Then you can run help (?) on each function and see the manual pages.

## Functions and Examples: Adult Data
```{r include=TRUE, echo = FALSE}
gender <- c(1,1,1,1,0,0,0,1,0,1,1,1,0,1,1,1,1,1,1,0,1,0,1,1,0,1,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,0,1,1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,1,1,1,0,1,1,1,1,1,1,0,1,1,1,0,1,0,0,1,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,0,0,1,1,0,1,0,1,1,1,1,0,0,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,0,0,1,1,0,1,1,0,1,1,1,0,1,0,1,0,1,0,1,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,1,0,1,1,1,0,1,1,1,1,1,0,1,1,1,1,1,0,1,1,1,1,0,0,0,1,0,1,1,1,1,1,1,0,1,1)
age <- c(39,50,38,53,28,37,49,52,31,42,37,30,23,32,40,34,25,32,38,43,40,54,35,43,59,56,19,54,39,49,23,20,45,30,22,48,21,19,31,48,31,53,24,49,25,57,53,44,41,29,25,18,47,50,47,43,46,35,41,30,30,32,48,42,29,36,28,53,49,25,19,31,29,23,79,27,40,67,18,31,18,52,46,59,44,53,49,33,30,43,57,37,28,30,34,29,48,37,48,32,76,44,47,20,29,32,17,30,31,42,24,38,56,28,36,53,56,49,55,22,21,40,30,29,19,47,20,31,35,39,28,24,38,37,46,38,43,27,20,49,61,27,19,45,70,31,22,36,64,43,47,34,33,21,52,48,23,71,29,42,68,25,44,28,45,36,39,46,18,66,27,28,51,27,28,27,21,34,18,33,44,43,30,40,37,34,41,53,31,58,38,24,41,47,41,23,36,40,35,24)
```
**Gender**: binary vector of size 200 with 0 = Female and 1 = Male

**Age**: vector of size 200 containing the corresponding ages


#### ls_optim(y, X)
The least squares optimizer uses a standard approach to a regression analysis. The ls_optim function works on finding how gender affects age.
```{r ls_optim1, include = FALSE, echo = TRUE}

library(boot)

ls_obj <- function(y, X)
{
  y <- as.vector(y)
  X <- as.matrix(X)
  solve(t(X)%*%X)%*%t(X)%*%y
}

loss <- function(y, X, beta)
{
  p <- c()
  bh <- c()
  n <- length(y)

  for (i in 1:n)
  {
    p[i] <- 1 / (1 + exp(t(-X[i, ]) %*% beta))
  }

  for (i in 1:n)
  {
    bh[i] <- (-y[i] * log(p[i]) - (1 - y[i]) * log(1 - p[i]))
  }

  return(sum(bh))
}

ls_optim <- function(y, X)
{
  n <- length(y)
  X <- cbind(X, c(rep(1, n)))
  beta_est <- optim(par = ls_obj(y, X), loss, y = y, X = X)$par

  out <- list("beta_hat" = beta_est, "response" = y, "predictors" = X)
  return(out)
}
```

```{r ls_optim2, include = TRUE, echo = TRUE}

ls_optim(gender, age)

```

#### bootstrap_ci(y, X, alpha = .05, rounds = 20)
The bootstrap intervals helps to focus on random sampling with replacement.
```{r bootstrap_ci1, include = FALSE, echo = TRUE}

bootstrap_ci <- function(y, X, alpha = .05, rounds = 20)
{
  beta_mat <- matrix(NA, rounds, 2)
  data <- data.frame("y" = y, "X" = X)
  for (b in 1:rounds)
  {
    boot_data <- data[sample(1:nrow(data), nrow(data), replace = TRUE), ]
    y2 <- boot_data[ , 1]
    X2 <- boot_data[ , 2]
    beta_mat[b, ] <- ls_optim(y2, X2)$beta_hat
  }
  boot_ci_int <- quantile(beta_mat[ , 1], c(alpha/2, 1-alpha/2))
  boot_ci_slope <- quantile(beta_mat[ , 2], c(alpha/2, 1-alpha/2))
  return(list("Bootstrap Confidence Interval For Intercept Coefficient"=boot_ci_int, "Bootstrap Confidence Interval For Slope Coefficient"=boot_ci_slope))
}

```
```{r bootstrap_ci2, include = TRUE, echo = TRUE}

bootstrap_ci(gender, age, alpha = 0.05, rounds = 20)

```


#### conf_matrix(y, X, alpha = .05, cutoff = 0.5)
The confusion matrix shows the performance of the ls_optim algorithm in a table. 
```{r conf_matrix1, include = FALSE, echo = TRUE}

conf_matrix <- function(y, X, alpha = .05, cutoff = 0.5)
{


  int_coeff <- mean(bootstrap_ci(y, X, alpha)$"Bootstrap Confidence Interval For Intercept Coefficient")
  slope_coeff <- mean(bootstrap_ci(y, X, alpha)$"Bootstrap Confidence Interval For Slope Coefficient")

  log_odds <- int_coeff + slope_coeff * X
  probabilities <- exp(log_odds)/(1+exp(log_odds))
  predict <- ifelse(probabilities > cutoff, yes = 1, no = 0)

  tp <- 0 # Number of true positives
  fn <- 0 # Number of false negatives
  fp <- 0 # Number of false positives
  tn <- 0 # Number of true negatives

  for (i in 1:length(y))
  {
    if (y[i] < .5 & predict[i] < .5)
    {
      tn <- tn +1
    }
    if (y[i] < .5 & predict[i] > .5)
    {
      fp <- fp +1
    }
    if (y[i] > .5 & predict[i] > .5)
    {
      tp <- tp +1
    }
    if (y[i] > .5 & predict[i] < .5)
    {
      fn <- fn +1
    }
  }

  row1 <- c(tp, fn)
  row2 <- c(fp, tn)
  cm <-  rbind(row1, row2)# confusion matrix
  lab <- c(1,0)
  rownames(cm) <- lab
  colnames(cm) <- lab

  prev <- (tp+fp)/(tp+fp+fn+tn)
  acc <- (tp+tn)/(tp+fp+tn+fn)
  tpr <- tp/(tp+fn)
  tnr <- tn/(tn+fp)
  fdr <- fp/(fp+tp)
  dor <- (tpr/(1-tnr))/((1-tpr)/tnr)


  return(list("Confusion Matrix" = cm, "Prevalence" = prev, "Accuracy" = acc, "Sensitivity" = tpr, "Specificity" = tnr, "False Discovery Rate" = fdr, "Diagnostic Odds Ratio" = dor))
}

```
```{r conf_matrix2, include = TRUE, echo = TRUE}

conf_matrix(gender, age, alpha = 0.05, cutoff = 0.5)

```

#### log_curve(X, y)
The log curve uses the ls_optim function to demonstrate a logistic curve in a plot.
```{r log_curve1, include = FALSE, echo = TRUE}
log_curve<- function(X, y)
{

  #fit logistic regression model
  model <- ls_optim(y,X)

  #define new data frame that contains predictor variable
  newdata <- data.frame(X=seq(min(X), max(X),len=100))

  #use fitted model to predict values
  p <- as.vector((model$beta_hat[1,])%*%(X))

  #plot logistic regression curve
  plot(y ~ X, col="steelblue", main = "Logistic Regression Curve", xlab= "X", ylab = "p")
  lines(p ~ X)

}
```

```{r log_curve2, include = TRUE, echo = TRUE}
log_curve(age, gender)
```

#### plot_metrics(y, X, alpha = 0.05)
The plot metrics uses the conf_matrix function to show the plot of accuracy in the confusion metrics. 
```{r plot_metrics1, include = FALSE, echo = TRUE}

plot_metrics <- function(y, X, alpha = 0.05)
{

  cut_off <- seq(0.1, 0.9, by = 0.1)
  acc1 <- conf_matrix(y,X,alpha, cutoff = 0.1)
  acc2 <- conf_matrix(y,X,alpha, cutoff = 0.2)
  acc3 <- conf_matrix(y,X,alpha, cutoff = 0.3)
  acc4 <- conf_matrix(y,X,alpha, cutoff = 0.4)
  acc5 <- conf_matrix(y,X,alpha, cutoff = 0.5)
  acc6 <- conf_matrix(y,X,alpha, cutoff = 0.6)
  acc7 <- conf_matrix(y,X,alpha, cutoff = 0.7)
  acc8 <- conf_matrix(y,X,alpha, cutoff = 0.8)
  acc9 <- conf_matrix(y,X,alpha, cutoff = 0.9)
  accuracy <- cbind(acc1$Accuracy, acc2$Accuracy, acc3$Accuracy, acc4$Accuracy, acc5$Accuracy, acc6$Accuracy, acc7$Accuracy, acc8$Accuracy, acc9$Accuracy)
  plot(cut_off, accuracy, main= "Accuracy vs Cutoff value", xlab = "Cutoff", ylab = "Accuracy")
  lines(cut_off, accuracy)

}

```
```{r plot_metrics2, include = TRUE, echo = TRUE}

plot_metrics(gender, age, alpha = 0.05)

```

